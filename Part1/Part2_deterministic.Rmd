---
title: "Bayesian Statistics with R-INLA - Part 2"
subtitle: 'Geilo, January, 2023'
author: "Sara Martino"
output:
  beamer_presentation:
    slide_level: 2
   # toc: true  
    keep_tex: yes  
    theme: "Singapore"
    colortheme: "default"
    font: "serif"
    includes:
      in_header: header.tex
---


```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = here::here("Part1"))

knitr::opts_chunk$set(echo = FALSE,
                      message=FALSE,
                      warning=FALSE,
                      strip.white=TRUE,
                      prompt=FALSE,
                      fig.align="center",
                       out.width = "60%")

library(knitr)    # For knitting document and include_graphics function
library(ggplot2)  # For plotting
library(png)
library(tidyverse)
library(INLA)
library(BAS)
library(patchwork)
```

## Outline
\tableofcontents[hideallsubsections]

# Recap

## Latent Gaussian models
Models of the kind:
$$
\begin{aligned}
\theta & \sim \pi(\theta)\\
\mathbf{x}|\theta& \sim \pi(\mathbf{x}|\theta) = \mathcal{N}(0, \mathbf{Q}^{-1}(\theta))\\
\mathbf{y}|\mathbf{x},\theta & \sim \prod_i\pi(y_i|\eta_i,\theta)
\end{aligned}
$$
occurs in many, seemingly unrelated, statistical models.


## Main Characteristics

1. Latent **Gaussian** model
2. The latent field has a **sparse** precision matrix (Markov properties)
3. The data are conditionally independent given the latent field
4. The predictor is linear


# Markov Properties

## Precision Matrix

We always talk about the precision matrix

$$
\mathbf{Q}(\theta) = \Sigma(\theta)^{-1}
$$
* Interpretaion of the elements of $\mathbf{Q}$

$$
\begin{aligned}
\end{aligned}
$$

* The precision matrix gives direct information about the conditional independence structure of the model

$$
x_i\perp x_j|\mathbf{x}_{-ij} \iff Q_{ij} = 0
$$

## Example - AR1 model

Consider an autoregressive process of order 1
$$
\begin{aligned}
x_t &= \phi x_{t-1} + \epsilon_t, \qquad \epsilon_t = \mathcal{N}(0,1),\qquad|\phi|<1\\
x_1&\sim\mathcal{N}(0, 1/(1-\phi^2))
\end{aligned}
$$
The joint distribution is:
$$
\begin{aligned}
\pi(\mathbf{x}) & =  \pi(x_1)\pi(x_2|x_1)\dots\pi(x_n|x_{n-1})\\
 & = \frac{1}{(2\pi)^{n/2}}|\mathbf{Q}|^{1/2}\exp\left(-\frac{1}{2}\mathbf{x}^T\mathbf{Qx}\right)
\end{aligned}
$$
<!-- can be written using its full conditional distributions as: -->
<!-- $$ -->
<!-- x_t|\mathbf{x}_{-t} = -->
<!-- \begin{cases} -->
<!-- \mathcal{N}(\phi x_{t+1},1), & t=1 \\ -->
<!-- \mathcal{N}(\frac{\phi}{1+\phi^2}(x_{t-1}+x_{t+1}),\frac{1}{1+\phi^2}), & 1<t<n\\ -->
<!-- \mathcal{N}(\phi x_{n-1},1), & t=n \\ -->

<!--  \end{cases}   -->
<!-- $$ -->

## Example - AR1 model
The precision matrix $\mathbf{Q}$ is:
$$
\mathbf{Q} =
\left(
  \begin{array}{ccccccc}
 1 & -\phi & & & & &\\
 -\phi & 1+\phi^2 & -\phi &&&& \\
  & & \ddots&\ddots&\ddots&&\\
 &&&&&&\\
 &&&&-\phi&1+\phi^2&-\phi\\
 &&&&&-\phi&1\\
\end{array} \right)
$$
\pause
The tridiagonal form is due to the fact that $x_i$ and $x_j$ are conditionally independent for $|i-j|>1$, given the rest.

## Example - AR1 models

Variance covariace function:

$$
\mathbf{\Sigma} =
\left(
  \begin{array}{ccccccc}
 1 & \phi & \phi^2& \phi^3&\dots & \dots& \phi^{n-1}\\
 \phi & 1 & \phi &\phi^2&\dots&\dots&\phi^{n-2} \\
  & & \ddots&\ddots&\ddots&&\\
 &&&&&&\\
 \phi^{n-2}&\phi^{n-3}&\dots&\dots&\phi&1&\phi\\
 \phi^{n-1}&\phi^{n-2}&\dots&\dots&\phi^2&\phi&1\\
\end{array} \right)
$$

## Precision Matrix



There are two main advantages in using (sparse) precision matrix over the variance/covariance matrix:

1. Building models through conditioning ("Hierarchical models")
2. Computational

## Building models through conditioning

If

* $\mathbf{x}\sim\mathcal{N}(0,\mathbf{Q}_x^{-1})$
* $\mathbf{y}|\mathbf{x}\sim\mathcal{N}(0,\mathbf{Q}_y^{-1})$

\pause
then

$$
 \mathbf{Q}_{(\mathbf{x},\mathbf{y})} =
        \begin{bmatrix}
            \mathbf{Q}_{x}+\mathbf{Q}_{y} & -\mathbf{Q}_{y} \\
            -\mathbf{Q}_{y} & \mathbf{Q}_{y}
        \end{bmatrix}
$$
We don't get so nice expressions with Covariance matrix

## Computational benefits

*  Models we have seen gives a sparse precision matrix
* These are much faster to compute with, than dense matrices
*  Special case: Kalman-filter algorithms

\pause
    Tasks:
    
* Factorize $\mathbf{Q}$ into $\mathbf{Q}=\mathbf{L}\mathbf{L}^{T}$ (Cholesky)
* Solve $\mathbf{Q}\mathbf{x} = \mathbf{b}$, $\mathbf{L}\mathbf{x}=\mathbf{b}$ or
        $\mathbf{L}^{T}\mathbf{x} = \mathbf{b}$
* Compute $\text{diag}(\mathbf{Q}^{-1})$

## Numerical algorithms for sparse matrices: scaling properties

Cholesky factorization of ``sparse'' SPD\footnote{Symmetric and positive definite} matrix

* Time: ${\mathcal O}(n)$
* Space: ${\mathcal O}(n^{3/2})$
* Space-time: ${\mathcal O}(n^2)$

\pause
 This is to be compared with general ${\mathcal O}(n^{3})$
    algorithm for the Cholesky factorization of a SPD dense matrix.

## Gaussian Markov random fields

* Gaussians with a sparse precision matrix are called
        \emph{Gaussian Markov random fields} (GMRFs)

* Good computational properties through numerical algorithms
        for sparse matrices

* Very useful for doing MCMC-based inference as well

## Summary

Three main ingredients in INLA

* Gaussian Markov random fields
* Latent Gaussian models
* Laplace approximations

    \pause

which together (and ++++...)  gives a very very nice tool for Bayesian inference:

* quick
* accurate (relative error)
* good scaling properties
* +++

# Deterministic inference for Gaussian models

## Computations
\Large
\
\
So...\
\
Now we have a modelling framework...\
\
But how do we get our answers?
\normalsize



## What do we care about?

\textcolor{red}{It depends on the problem!}

>- A single element of the latent field (e.g.the sign or quantiles of a fixed effect)
>- A linear combination of elements from the latent field (the average over an	area of a spatial effect, the difference of two effects)
>- A single hyperparameter (the correlation)
>- A non-linear combination of hyper parameters (animal models)
>- Predictions at unobserved locations


## What do we care about?

The most important quantity in Bayesian statistics is \textcolor{red}{the posterior distribution}:
$$
\begin{aligned}
\overbrace{\pi(\mathbf{x}, {\theta}\mid\mathbf{y})}^{{\text{Posterior}}} &\propto \overbrace{\pi({\theta}) \pi(\mathbf{x}\mid{\theta})}^{{\text{Prior}}} \overbrace{\prod_{i \in \mathcal{I}}\pi(y_i \mid x_i, {\theta})}^{{\text{Likelihood}}}
\end{aligned}
$$
	from which we can derive the quantities of interest, such as
$$
	\begin{aligned}
		{\pi(x_i \mid \mathbf{y})} &\propto \int \int \pi(\mathbf{x}, {\theta}\mid\mathbf{y}) d{x}_{-i} d{\theta}\\
		&= {\int \pi(x_i \mid {\theta}, \mathbf{y}) \pi({\theta} \mid \mathbf{y}) d{\theta}}
	\end{aligned}
$$
or $\pi(\theta_j\mid \mathbf{y})$.

These are very high dimensional integrals and are typically not
analytically tractable.



## Traditional approach: MCMC

MCMC is based on sampling with the goal to \textcolor{red}{construct a Markov chain with the target posterior as stationary distribution}.

* Extensively used within Bayesian inference since the 1980's.
* Flexible and general, sometimes the only thing we can do!
* A generic tool is available with \texttt{JAGS}/\texttt{OpenBUGS}.
* Tools for specific models are of course available, e.g.~\texttt{BayesX} and \texttt{stan}.
* Standard MCMC sampler are generally easy-ish to program and are in fact implemented in readily available software
*  However, depending on the complexity of the problem, their efficiency might 	be limited.


## Approximate inference

Bayesian inference can (almost) never be done exactly. Some form of
approximation must always be done.

* MCMC ``works'' for everything, but it can be incredibly
			slow
* Is it possible to make a quicker, more specialized	inference scheme which only needs to work for this limited		class of models? (specifically LGM)



## Recall: What is our model framework?

Latent Gaussian models

$$
\begin{aligned}
y| x, \theta & \sim \prod  \pi(y_i|x_i,\theta) & \\
x|\theta & \sim\mathcal{N}(0,Q(\theta)) & \text{\textcolor{red}{Gaussian!!!}}\\
\theta & \sim\pi(\theta)& \text{Not Gaussian}
\end{aligned}
$$

where the precision matrix $Q(\theta)$ is sparse. Generally
these ``sparse'' Gaussian distributions are called \textcolor{red}{Gaussian Markov random fields} (GMRFs).

The sparseness can be exploited for very quick computations for the Gaussian	part of the model through numerical algorithms for sparse matrices.

## The INLA idea

Use the properties of the LGM we have defined to
approximate the posterior \textcolor{red}{marginals}

$$
\begin{aligned}
		\pi(x_i \mid \boldsymbol{y})\quad \text{and} \quad \pi(\theta_j \mid \boldsymbol{y})
	\end{aligned}
$$
directly. \
\
Let us consider a \textcolor{red}{toy example to illustrate the ideas}.



## How does INLA work? A toy example

Smoothing noisy observations - Data

We observe some smooth function but our measures are noisy (but we know the size of such noise!)

```{r}
n = 50
idx = 1:50
x = idx
func = 100 * ((idx-n/2)/n)^3
y = func + rnorm(n,0,1)
data.frame(x = x, y = y, f = func) %>% ggplot() + geom_point(aes(x,y)) +
  geom_line(aes(x,f), color = "red")
```

**Goal:** Recover the smooth function observed with noise!

## Smoothing noisy observations - Model

Assume:
\begin{align*}
y_i &= f(i) + \epsilon_i; i = 1,\dots,n \\\nonumber
\epsilon_i&\sim N(0,1) \\\nonumber
f(i) &= x_i\text{ smooth function of } i\nonumber
\end{align*}

-  Only one hyperparameter

- Gaussian likelihood

\textcolor{red}{Is this a Latent Gaussian model?}

## Smoothing noisy observations - LGM

- **Data** Gaussian Observations with known precision
$$
    y_i|x_i\sim\mathcal{N}(x_i,1)
$$

- **Latent Model**: A Gaussian model for the smooth function (RW2 model)
$$
    \pi({\mathbf x}|\theta)\propto \theta^{(n-2)/n}\exp\left\{
    -\frac{\theta}{2}\sum_{i=2}^n(x_i-2x_{i-1}+x_{i-2})^2
    \right\}
$$

- **Hyperparameter** The precision of the smooth function $\theta$. We assign a Gamma prior

$$
    \pi(\theta)\propto\theta^{a-1}\exp(-b\theta)
$$


## Smoothing noisy observations - Goal

Find approximations for:

1. The posterior marginal for the hyperparameter $\pi(\theta|\mathbf{y})$
1. The posterior marginals for the elements of the latent field $\pi(x_i|\mathbf{y})$



## Approximating $\pi(\theta|\mathbf{y})$

We have that
$$
\pi(\mathbf{x},\theta,\mathbf{y}) = \pi(\mathbf{x}|\theta,\mathbf{y})\pi(\theta|\mathbf{y})\pi(\mathbf{y})
$$

so

$$
    \pi(\theta|\mathbf{y}) = \frac{\pi(\mathbf{x},\theta,\mathbf{y})}{\pi(\mathbf{x}|\theta,\mathbf{y})\pi(\mathbf{y})} \propto\frac{
  \pi(\mathbf{y}, \mathbf{x}|\theta)\  \pi(\theta)
    }{\pi(\mathbf{x}|\theta,\mathbf{y})}
$$

\pause

Since the likelihood is Gaussian, then $\pi(\mathbf{y}, \mathbf{x}|\theta)$
is also Gaussian. We have then:

$$
   \pi(\theta|\mathbf{y})  \propto \frac{
  \overbrace{\pi(\mathbf{y}, \mathbf{x}|\theta)}^{\text{Gaussian}}\  \pi(\theta)}
  {\underbrace{\pi(\mathbf{x}|\theta,\mathbf{y})}_{\text{Gaussian}}}
$$
This is valid for any $\mathbf{x}$


## Posterior marginal for the hyperparameter

Select a grid of points to represent the density $\pi(\theta|\mathbf{x})$
```{r}
library(patchwork)
R = matrix(0,n,n)
diag(R) = c(1,rep(2,n-2),1)
for(i in 2:n)
  R[i-1,i] =R[i,i-1] = -1

llik = function(y,x)
  return(0.5*(y-x)^2)

lpriorX = function(x,theta)
  return(-theta/2*t(x)%*%R%*%x)

lpriorTheta = function(theta)
  {
    a = 10
    b = 10
    return((a-1)*log(theta)-b*theta)
}

lfullcond = function(y,theta,x)
{
  Q = theta * R + diag(n)
  b = t(y) %*% diag(n)
  return(-0.5*t(x)%*%Q%*%x+b%*%x)
}

theta = seq(0.01,2,length.out = 15)
lpost = numeric(length(theta))
for(i in 1:length(theta))
  lpost[i] = sum(llik(y,x)) +
    lpriorX(x,theta[i]) +
    lpriorTheta(theta[i]) -
  lfullcond(y,theta[i],x)
lpost= lpost-max(lpost)
p1 = data.frame(theta=theta, f=exp(lpost)) %>%
  ggplot(aes(theta,f)) + geom_point() +
  scale_x_continuous(name = expression(theta)) +
  scale_y_continuous(name = "density")  +
  ggtitle(bquote('Posterior marginals for'~theta ))
p2 = data.frame(theta=theta, f=exp(lpost)) %>%
  ggplot(aes(theta,f)) + geom_point() + geom_line() +
  scale_x_continuous(name = expression(theta)) +
  scale_y_continuous(name = "density")  +
  ggtitle(bquote('Posterior marginals for'~theta ))

p1+p2
```


## Approximating $\pi(x_i|y,\theta)$
 Again we have that
$$
    \mathbf{x},\mathbf{y}|\theta\sim\mathbf{N}(\cdot,\cdot)
$$
  so also $\pi(x_i|\theta,\mathbf{y})$ is Gaussian!!\

  We compute
\begin{align*}
\pi(x_i|{\mathbf y}) &= \int \pi(x_i|\theta,{\mathbf y})\pi(\theta|{\mathbf y})d\theta\\
      &\approx \sum_k\pi(x_i|\theta_k,{\mathbf y})\pi(\theta_k|{\mathbf y}) \Delta_k
\end{align*}
where $\theta_k,k=1,\dots,K$ are the representative points of
  $\pi(\theta|\mathbf{y})$ and $\Delta_k$ are the corresponding weights


## Posterior marginals for latent field I

Compute the conditional posterior marginal for $x_i$ given each $\theta_k$\
```{r}
xval = seq(1,9,0.01)
densX = matrix(0,length(xval), length(theta))

for(j in 1:length(theta))
{
  for(i in 1:length(xval))
  {
    x[10] = xval[i]
    densX[i,j] = lfullcond(y,theta[j],x)
  }
  densX[,j] =exp(densX[,j]-max(densX[,j]))
}

c = apply(densX,2,sum)*0.1
for(i in 1:length(theta))
  densX[,i] = densX[,i]/c[i]
data.frame(x = xval, f = densX[,-c(1:4)]) %>%
   gather("name","val",-x) %>%
   ggplot() +  geom_line(aes(x,val, group=name)) +
   scale_x_continuous(name = "x") +
   scale_y_continuous(name = "density")  +
   ggtitle(bquote('Posterior marginals forv'~x[10] ~' for each' ~theta ~'(unweighted)'))
```

## Posterior marginals for latent field II
Weight the conditional posterior marginal for
    $\pi(x_i|\theta_k, \mathbf{y})$ by
    $\pi(\theta_k|\mathbf{y})\Delta_k$
```{r}


densX_weight = densX
for(i in 1:length(theta))
  densX_weight[,i] = densX[,i]*exp(lpost[i])*diff(theta)[1]

data.frame(x = xval, f = densX_weight) %>%
   gather("name","val",-x) %>%
   ggplot(aes(x,val, group=name)) +  geom_line() +
   scale_x_continuous(name = "x") +
   scale_y_continuous(name = "density")  +
   ggtitle(bquote('Posterior marginals forv'~x[10] ~' for each' ~theta ~'(weighted)'))
```

## Posterior marginals for latent field III

 Sum to get the posterior marginal for $x_i|\mathbf{y}$
```{r}
  data.frame(x = xval, f = densX_weight) %>%
  mutate(tot.1 = rowSums(.[-1])) %>%
  gather("name","val",-x) %>%
  separate(name, c("A", "B"), remove=FALSE) %>%
  ggplot(aes(x,val, group=name, color = A)) +  geom_line() +
  scale_x_continuous(name = "x") +
  scale_y_continuous(name = "density")  +
  ggtitle(bquote('Posterior marginals for '~x[10] )) +
  theme(legend.position="none")

```


## Fitted Spline
  The posterior marginals are used to calculate summary statistics, like means, variances and credible intervals:

```{r, out.width="60%"}
data = data.frame(y = y, idx=idx)
hyper1 = list(prec=list(initial=0, fixed=T))
#hyper2 = list(prec =list(param=c(10,10)))
res = inla(y~f(idx, model = "rw2"),
           data = data,
           control.family = list(hyper = hyper1))


data.frame(res$summary.random$idx) %>%
  dplyr::select("ID", "mean", "X0.025quant", "X0.975quant") %>%
  ggplot(aes(ID,mean)) + geom_line( color="red") +
  ggtitle("Posterior mean and quantiles of the smooth effect") +
  geom_point(data = data.frame(id=1:n,y=y),
             mapping = aes(x = id, y = y))
```

## `R-INLA` code

\small

```{r, echo = T, eval=FALSE}

formula = y ~ -1 + f(idx, model="rw2", constr=FALSE,
   hyper=list(prec=list(prior="loggamma", param=c(a,b))))

result = inla(formula,
      data = data.frame(y=y, idx=idx),
      control.family = list(initial = log(tau_0), fixed=TRUE))
```
\normalsize

This exercise is contained in the `01_Practical_implement_INLA.html` file.

# Extending the method

## Extending the method

This is the basic idea behind INLA. It is quite simple.

However, we need to extend this basic idea so we can deal with

1. Non-Gaussian observations

2. More than one hyperparameter

## 1. More than one hyperparameter

\textcolor{red}{Main use:} Select good evaluation points ${\theta}_k$ for the numerical integration when approximating $\widetilde{\pi}(x_i|{y})$

- Locate the mode
  \includegraphics[width=5cm]{./graphics/ellipse1}

## 1. More than one hyperparameter

\textcolor{red}{Main use:} Select good evaluation points ${\theta}_k$ for the numerical integration when approximating $\widetilde{\pi}(x_i|{y})$

- Locate the mode
- Compute the Hessian to construct principal components

  \includegraphics[width=5cm]{./graphics/ellipse2}


## 1. More than one hyperparameter

\textcolor{red}{Main use:} Select good evaluation points ${\theta}_k$ for the numerical integration when approximating $\widetilde{\pi}(x_i|{y})$

- Locate the mode

- Compute the Hessian to construct principal components

- Grid-search to locate bulk  of the probability mass
  \includegraphics[width=5cm]{./graphics/ellipse3}


## 1. More than one hyperparameter


- Locate the mode

- Compute the Hessian to construct principal components

- Grid-search to locate bulk  of the probability mass

- For each point $k$ in the grid compute:

    - $\widetilde{\pi}(\theta^k|y)$
    - $\widetilde{\pi}(x_i|\theta^k,y)$
    - $\Delta_k$


## 2. Non-Gaussian observations

In application we may choose likelihoods other than a
Gaussian. How does this change things?

$$
\pi(\mathbf{\theta} \mid \mathbf{y}) \propto \frac{
            \overbrace{\pi(\mathbf{x}, \mathbf{y}\mid \mathbf{\theta})}^{\text{Non-Gaussian, BUT KNOWN}}
        \; \pi(\mathbf{\theta})}{\underbrace{\pi(\mathbf{x} \mid \mathbf{y},
            \mathbf{\theta})}_{\text{Non-Gaussian and UNKNOWN}}}
$$

* In many cases	\(\pi(\boldsymbol{x} \mid \boldsymbol{y}, \boldsymbol{\theta})\)is very close to a Gaussian distribution, and can be replaced	with a \textcolor{red}{Laplace approximation}.



## The GMRF (Laplace) approximation

Let $\mathbf{x}$ denote a GMRF with precision matrix $\mathbf{Q}$ and mean $\mathbf{\mu}$.

Approximate
$$
\begin{aligned}
\pi(\mathbf{x}|\theta,\mathbf{y}) &\propto
            \exp\left(-\frac{1}{2}\mathbf{x}^\top \mathbf{Q}\mathbf{x} + \sum_{i=1}^n \log \pi (y_i|x_i)\right)
\end{aligned}
$$
by using a second-order Taylor expansion of $\log \pi (y_i|x_i)$ around $\mathbf{\mu}_0$, say.

* Recall
$$
\begin{aligned}
        f(x) \approx f(x_0) + f'(x_0)(x-x_0)+ \frac{1}{2} f''(x_0)(x-x_0)^2
        = a+ bx - \frac{1}{2}cx^2
\end{aligned}
$$
with 
$$
\begin{aligned}
b &=f'(x_0) - f''(x_0)x_0\\
c &= -f''(x_0)
\end{aligned}
$$. 

(Note: $a$ is not relevant).

## The GMRF approximation (II)

Thus,
$$
\begin{aligned}
        \widetilde{\pi}(\mathbf{x}|\mathbf{\theta}, \mathbf{y}) &\propto
            \exp\left(-\frac{1}{2}\mathbf{x}^\top \mathbf{Q}\mathbf{x}  +
            \sum_{i=1}^n (a_i + b_i x_i - 0.5 c_i x_i^2)\right)\\
        &{\propto \exp\left(-\frac{1}{2}\mathbf{x}^T(\mathbf{Q} + \text{diag}(\mathbf{c})) \mathbf{x} + \mathbf{b}^T\mathbf{x}\right)}
\end{aligned}
$$

which is Gaussian with precision matrix $\mathbf{Q} + \text{diag}(\mathbf{c})$ and mean given by the solution of $(\mathbf{Q} + \text{diag}(\mathbf{c}))\mathbf{\mu} = \mathbf{b}$

\textcolor{red}{The canonical parameterisation} is
$$
\textcolor{red}{\mathcal{N}_C(\mathbf{b}, \mathbf{Q} + \text{diag}(\mathbf{c}))}
$$
which corresponds to
$$
\mathcal{N}((\mathbf{Q} + \text{diag}(\mathbf{c}))^{-1}\mathbf{b}, (\mathbf{Q} + \text{diag}(\mathbf{c}))^{-1}).
$$

## The GMFR approximation - One dimensional example

Assume
$$
\begin{aligned}
  y|\lambda \sim\text{Poisson}(\lambda)  & \text{ Likelihood}\\
  \lambda = \exp(x)  & \text{ Likelihood}\\
  x\sim\mathcal{N}(0,1) & \text{ Latent Model}
\end{aligned}
$$
  we have that
$$
  \pi(x|y)\propto\pi(y|x)\pi(x)\propto\exp\{ -\frac{1}{2}x^2+
  \underbrace{xy-\exp(x)}_{\text{non-gaussian part}}
  \}
$$

## The GMRF approximation

```{r}
source("GMRF_approx.R")

df = data.frame(x = xi,
                  true = fc(xi, y, eta,delta)/nominator,
                  app1  = NA,
                  app2 = NA,
                  app3 = NA,
                  app4 = NA
                  )
mode =  xi[which.max(fc(xi, y, eta,delta)/nominator)]
   xi_0 = c(0,0.5,1,1.5)

exp.points = data.frame(mode =  rep(mode,4),
                          exp.point = xi_0)

for(i in 1:4)
  {
    gmrf <- GMRF_approx(xi, xi_0[i], y, eta, delta)
    df[,i+2] = dnorm(xi, gmrf$b/gmrf$c, sqrt(1/gmrf$c))
  }

p1 = ggplot() + geom_line(data  = df, aes(x,true))  +
   geom_line(data = df, aes(x, app1), color = "red") +
   xlab("") + ylab("") + ggtitle(paste("Expansion around", xi_0[1])) +
  geom_point(aes(x=exp.points[1,1], y=0), size = 2) +
   geom_point(aes(x=exp.points[1,2], y=0), color = "red", size = 2)
p2 = ggplot() + geom_line(data  = df, aes(x,true))  +
   geom_line(data = df, aes(x, app2), color = "red") +
   xlab("") + ylab("") + ggtitle(paste("Expansion around", xi_0[2])) +
  geom_point(aes(x=exp.points[2,1], y=0), size = 2) +
   geom_point(aes(x=exp.points[2,2], y=0), color = "red", size = 2)
p3 = ggplot() + geom_line(data  = df, aes(x,true))  +
   geom_line(data = df, aes(x, app3), color = "red") +
   xlab("") + ylab("") + ggtitle(paste("Expansion around", xi_0[3])) +
  geom_point(aes(x=exp.points[3,1], y=0), size = 2) +
   geom_point(aes(x=exp.points[3,2], y=0), color = "red", size = 2)
p4 = ggplot() + geom_line(data  = df, aes(x,true))  +
   geom_line(data = df, aes(x, app4), color = "red") +
   xlab("") + ylab("") + ggtitle(paste("Expansion around", xi_0[4])) +
  geom_point(aes(x=exp.points[4,1], y=0), size = 2) +
   geom_point(aes(x=exp.points[4,2], y=0), color = "red", size = 2)

p1+p2+p3+p4
```
 \textcolor{red}{If $\mathbf{y} \mid \mathbf{x}, \mathbf{\theta}$ is Gaussian "the approximation" is exact!}
}


## What do we get ...

$$
\widetilde{\pi}(\mathbf{\theta} \mid \mathbf{y}) \propto  \frac{
           \pi(\mathbf{x}, \mathbf{y}\mid \mathbf{\theta})
        \; \pi(\mathbf{\theta})}{\widetilde{\pi}_G(\mathbf{x} \mid \mathbf{y},
            \mathbf{\theta})} \: \Bigg|_{\mathbf{x} = \mathbf{x}^\star(\mathbf{\theta})}
$$

* find the mode of $\widetilde{\pi}(\mathbf{\theta}\mid \mathbf{y})$
        (optimization)

* explore $\widetilde{\pi}(\mathbf{\theta}\mid \mathbf{y})$ to find
        grid points $t_k$ for numerical integration.\
\
\pause
However, why is it called \textcolor{red}{integrated nested Laplace
      approximation}?
\pause
There is another step that changes:
$$
\pi(x_{i} \mid \mathbf{y}) \approx \sum_k \underbrace{\pi(x_{i} \mid \mathbf{y}, \theta^k)}_{{\text{Not Gaussian!}}}{\widetilde{\pi}_G(\theta^k\mid\mathbf{y})} \Delta_k
$$


## Approximating $\pi(x_i|\mathbf{y}, \mathbf{\theta})$

Three possible approximations:

>- 1. \textcolor{red}{Gaussian distribution}  derived from $\widetilde{\pi}_G(\mathbf{x}|\mathbf{\theta}, \mathbf{y})$, i.e.
$$
\widetilde{\pi}(x_i|\mathbf{\theta}, \mathbf{y}) = \mathcal{N}(x_i; \mu_i(\mathbf{\theta}), \sigma_i^2(\mathbf{\theta}))
$$
with mean $\mu_i(\mathbf{\theta})$ and marginal variance $\sigma_i^2(\mathbf{\theta})$.
\
However, errors in location and/or lack of skewness possible

>- 2. \textcolor{red}{Laplace approximation}
>- 3. \textcolor{red}{Simplified Laplace approximation}

## Laplace approximation of $\pi(x_i|\mathbf{\theta}, \mathbf{y})$

Use again the same idea! 

Based on the identity
$$
\pi(z) = \frac{\pi(x,z)}{\pi(x|z)}\quad \text{ leading to }\quad \tilde{\pi}(z) = \frac{\pi(x,z)}{\tilde{\pi}(x|z)}\
$$
When $\tilde{\pi}(x|z)$ is the Gaussian approximation, this is the Laplace approximation.

## Laplace approximation of $\pi(x_i|\mathbf{\theta}, \mathbf{y})$

$$
\widetilde{\pi}_\text{LA}(x_i|\mathbf{\theta}, \mathbf{y}) \propto
			\frac{\pi(\mathbf{x}, \mathbf{\theta},\mathbf{y})}
			{\widetilde{\pi}_\text{GG}(\mathbf{x}_{-i}|x_i, \mathbf{\theta}, \mathbf{y})}
			\Biggr|_{\mathbf{x}_{-i}=\mathbf{x^\star}_{-i}(x_i, \mathbf{\theta})}
$$

The approximation is very good but expensive as $n$ factorizations
of $(n-1) \times (n-1)$ matrices are required to get the $n$
        marginals.

\pause
\textcolor{blue}{Computational modifications exist:}

1.  Approximate the modal configuration of the GMRF
                  approximation.
2.  Reduce the size $n$ by only involving the ``neighbors''.

## Simplified Laplace approximation
Faster alternative to the Laplace approximation \

*  based on a \textcolor{red}{series
	expansion up to third order of the numerator and denominator of $\widetilde{\pi}_\text{LA}(x_i|\mathbf{\theta}, \mathbf{y})$}
*  corrects the Gaussian approximation for error in
                  location and lack of skewness.

\pause
This is \textcolor{red}{default option when using INLA} but this choice can be modified.


## INLA: When does it work

We consider models of the kind
$$
\begin{aligned}
\theta & \sim \pi(\theta)\\
\mathbf{x}|\theta& \sim \pi(\mathbf{x}|\theta) = \mathcal{N}(0, \mathbf{Q}^{-1}(\theta))\\
\mathbf{y}|\mathbf{x},\theta & \sim \prod_i\pi(y_i|\eta_i,\theta)
\end{aligned}
$$
where

* $\mathbf{x}$ can be large but endowed with Markov properties so that $\mathbf{Q}(\theta)$ is sparse

* the size of $\theta$ is small (say <15) 

* $\eta$ is a predictor that depends *linearly* on the other elemets of $\mathbf{x}$

* The main inferential interest lies in the posterior marginals $\pi(x_i|\mathbf{y})$, $\pi(\theta_j|\mathbf{y})$ rather than in the joint $\pi(\mathbf{x},\theta|\mathbf{y})$ (....but joint inference is possible through sampling!)

## INLA: Overview

- **Step I** Approximate $\pi({\theta}|y)$ using the Laplace approximation and select good evaluation points ${\theta}_k$.

- **Step II** For each ${\theta}_k$ and $i$ approximate $\pi(x_i|{\theta}_k, {y})$  using the Laplace or simplified Laplace approximation for selected values of $x_i$

- **Step III** For each $i$, sum out ${\theta}_k$
$$
\widetilde{\pi}(x_i|{y}) = \sum_k \widetilde{\pi}(x_i|{\theta}_k, {y}) \times
                    \widetilde{\pi}({\theta}_k|{y}) \times \Delta_k.
$$
Build a log spline corrected Gaussian to represent $\widetilde{\pi}(x_i|{y})$.

## INLA: Why does it work?

- The full conditional $\pi(x|y,\theta)$ is "almost" Gaussian

- The latent field $x$ is  a GMRF

  - GMRF $\rightarrow$ sparse precision matrix!!
  - Easy to solve and store

- Smart numerical methods

- Parallel implementation





## Limitations

 - The dimension of the latent field $x$ can be large ($10^2-10^6$)

 -  The dimension of the hyperparameters $\theta$ must be small
($\leq 9$)

In other words, each random effect can be big, but there cannot be too many random effects unless they share parameters.

## INLA: summary

* These are the basic ideas

* The rest are *just* details....but there are a lot of them!

## INLA features
INLA fully incorporates posterior uncertainty with respect to
hyperparameters $\Rightarrow$ tool for full Bayesian inference

*  Marginal posterior densities of all (hyper-)parameters
* Posterior mean, median, quantiles, std.~deviation, etc.
* The approach can be used for predictions, model assessment, \ldots
* Joint posterior marginal not available...but it is possible to sample from $\widetilde{\pi}(x,\theta|y)$


