---
title: "Bayesian Statistics with R-INLA - Part 1"
subtitle: 'Geilo, January, 2023'
author: "Sara Martino"
output:
  beamer_presentation:
    slide_level: 2
    keep_tex: yes  
    theme: "Singapore"
    colortheme: "default"
    font: "serif"
    includes:
      in_header: header.tex
---


<!-- rmarkdown::render("1Intro/1Intro.Rmd","all",encoding="UTF-8") -->
<!-- rmarkdown::render("1Intro/1Intro.Rmd","html_document",encoding="UTF-8") -->
<!-- rmarkdown::render("1Intro/1Intro.Rmd","pdf_document",encoding="UTF-8") -->
<!-- rmarkdown::render("1Intro/1Intro.Rmd","beamer_presentation",encoding="UTF-8") -->


```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = here::here("Part1"))

knitr::opts_chunk$set(echo = FALSE,
                      message=FALSE,
                      warning=FALSE,
                      strip.white=TRUE,
                      prompt=FALSE,
                      fig.align="center",
                       out.width = "60%")

library(knitr)    # For knitting document and include_graphics function
library(ggplot2)  # For plotting
library(png)
library(tidyverse)
library(INLA)
library(BAS)
library(patchwork)
```


## Outline
\tableofcontents[hideallsubsections]


# Introduction

## What is inla?

**The short answer:**\
\

> INLA is a fast method to do approximate Bayesian inference with
latent Gaussian models and `INLA` is an `R`-package that
implements this method with a flexible and simple interface.

\pause

**The (much) longer answer:**

* Rue, Martino, and Chopin (2009) "Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations." *JRSSB*
* Rue, Riebler, SÃ¸rbye, Illian, Simpson, Lindgren (2017) "Bayesian Computing with INLA: A Review." *Annual Review of Statistics and Its Application*
* Martino, Riebler "Integrated Nested Laplace Approximations (INLA)" (2021) *arXiv:1907.01248*

## Where?

The software, information, examples and help can be found at `http://www.r-inla.org`

```{r, out.width = "50%"}
# All defaults
include_graphics("graphics/rinla.png")
```

* paper
* tutorials
* discussion group
* ...

## So... Why should you use `R-INLA`?

* Why approximate inference?
* What type of problems and models can we solve?
* When can we use it?\
\
\

## Bayesian Inference

* **Likelihood** $\pi(\mathbf{y}|\mathbf{\theta})$

* **Prior** $\pi(\mathbf{\theta})$

* **Posterior**
$$
\pi(\mathbf{\theta}|\mathbf{y}) = \frac{\pi(\mathbf{y},\mathbf{\theta}) }{\pi(\mathbf{y})}\propto \pi(\mathbf{y}|\mathbf{\theta})\pi(\mathbf{\theta})
$$

## Background I

>- A big obstacle for Bayesian modelling is the issue of "Bayesian Computing"

>- MCMC is a very general solution but can be slow, can have convergence problems ...

>- There are (somewhat) generic tools based on MCMC like `JAGS/OpenBUGS/stan`...

## Background II

GLM/GAM/GLMM/GAMM/++

>- Perhaps the most used class of statistical models

>- All these models can be seen as Bayesian hyperarchical models...

>- ...more specifically then can be cast into the class of "Latent Gaussian Models"

>- For such "restricted" class INLA beats MCMC in terms of velocity and accuracy.


# Bayesian Hierarchical models

## Build a Bayesian model, a simple example

1. We observed something...

```{r,  out.width = "90%"}
data("bodyfat")
p1 = bodyfat %>% ggplot() + geom_point(aes(Abdomen,Bodyfat))
p1
```

## Build a Bayesian model, a simple example

2. We formulate questions ..

```{r}
my_image <- readPNG("graphics/question.png", native = TRUE)
p1  +
  annotate("text", x=120, y=10, label="Does bodyfat vary\n with abdomen circunference??", size = 6,
              color="red")+
  inset_element(p = my_image,
                left = 0.5,
                bottom = 0.55,
                right = 0.95,
                top = 0.95)
```

## Build a Bayesian model, a simple example

3. We formulate a model to answer the questions ..



* The observational model
* How data depend on each other/ or other quantities
* What is our "information" prior to the observation process

## A Bayesian regression model

* Perc. Body fat $(y_1, \dots, y_n)$ are Gaussian distributed
  - The mean $\eta_i$ depends on the abdomen circunference
  - The precision is constant

$$
\begin{aligned}
  y_i|\eta_i,\tau &\sim \mathcal{N}(\eta_i,\tau^{-1})\\
  \eta_i & =  \alpha + \beta x_i
\end{aligned}
$$

* Need priors for $\alpha$, $\beta$, $\tau$

  * $(\alpha, \beta)\sim\mathcal{N}(0, \text{diag}(\sigma^2_{\alpha}, \sigma^2_{\beta}))$ with $\sigma^2_{\alpha}, \sigma^2_{\beta}$ known

  * $\tau\sim\text{Gamma}(a,b)$ with $a,b$ known


```{r,  out.width = "90%"}
data("bodyfat")
bodyfat$Abdomen = bodyfat$Abdomen - mean(bodyfat$Abdomen)
p1 = bodyfat %>% ggplot() + geom_point(aes(Abdomen,Bodyfat))
mod = lm(Bodyfat~ Abdomen, data = bodyfat)

Q = matrix(0, 102, 102)
diag(Q) = 1
Q[,c(101,102)] = 1
Q[c(101,102),] =1
Q <- apply(Q, 2, rev)
Q = t(Q)
Q.df <- reshape2::melt(Q, c("x", "y"), value.name = "z")
```



## A Bayesian hyerarchical model

* **Observation model**

$$
\mathbf{y} \mid \underbrace{\alpha,
            \beta}_{\mathbf{x}}, \underbrace{\tau}_{\mathbf{\theta}}
$$
Encodes information about observed data

* **Latent model**

$$
\mathbf{x} = (\alpha, \beta, \eta)\sim\mathcal{N}(0, \mathbf{Q}^{-1}(\theta))
$$

The unobserved process

* **Hyperparameters **  $\theta = \tau$

## Precision Matrix $\mathbf{Q}(\theta)$
```{r}
Qplot = ggplot(data=Q.df,aes(x=x,y=y,fill=factor(z)))+
  geom_tile() + theme_void() + theme(legend.position = "none") +
  scale_fill_manual(values = c('grey90', 'black' )) + 
  coord_equal()
Qplot
```


## Bayesian Computations

From this we can compute the \textcolor{red}{posterior distribution}
$$
\pi(\mathbf{x}, \theta | \mathbf{y}) \propto \pi(\mathbf{y} | \mathbf{x},
     \theta) \pi(\mathbf{x}) \pi(\theta)
$$
and then the corresponding \textcolor{red}{posterior marginal distributions}:
$$
\begin{aligned}
  \pi(x_j|\mathbf{y}) &\qquad  j = 1,2\\
  \pi(\tau|\mathbf{y})& \\
  \pi(\eta_i|\mathbf{y}) &\qquad  i = 1,\dots,n
\end{aligned}
$$




## Results

* Assign priors to $\alpha,\beta,\tau$
* Use Bayes theorem to compute posterior distributions
```{r}
out = inla(Bodyfat~Abdomen, data  = bodyfat, control.compute=list(config = TRUE),
           control.predictor=list( compute=TRUE))
```

```{r}

pred = data.frame(out$summary.linear.predictor) %>%
  mutate(obs = bodyfat$Bodyfat,
         x = bodyfat$Abdomen) %>%
  arrange(obs)


var = inla.tmarginal(function(x) 1/sqrt(x), out$marginals.hyperpar$`Precision for the Gaussian observations`)


p1 = ggplot(data = data.frame(out$marginals.fixed$`(Intercept)`), aes(x,y))+geom_line() + xlab("") + ylab("") + ggtitle("Intercept")
p2 = ggplot(data = data.frame(out$marginals.fixed$Abdomen), aes(x,y))+geom_line()+ xlab("") + ylab("") + ggtitle("Abdomen")
marg = out$marginals.hyperpar$`Precision for the Gaussian observations`
d1 = inla.tmarginal(function(x) 1/sqrt(x), marg)
p3 = ggplot(data = data.frame(d1), aes(x,y))+geom_line()+ xlab("") + ylab("") + ggtitle("Sd")
p4 = ggplot() + geom_point(data  = bodyfat, aes(Abdomen,Bodyfat)) +
  geom_line(data  = pred, aes(x, mean), color = "red") +
  geom_ribbon(data = pred, aes(x =x, ymin = X0.025quant, ymax = X0.975quant),
              fill = "red", alpha = 0.3)
p1+p2+p3+p4
```


## On the prior choice....

* Priors are an important part of the model
* There are several "schools" about priors
* "non-informative" priors are not always a good choice
* for complex models priors can have a large influence on the results.






## Real-world datasets are usually much more complicated!
Using a Bayesian framework:

* Build (hierarchical) models to account for potentially
		complicated dependency structures in the data.
* Attribute uncertainty to model parameters and latent variables
		using priors.

**Two main challenges:**

1. Need computationally efficient methods to calculate posteriors.
2. Select priors in a sensible way




## Bayesian hierarchical models

INLA can be used with Bayesian hierarchical models where
we model in different stages or levels:

* **Stage 1:** What is the distribution of the responses?\
\
* **Stage 2:** What is the distribution of the underlying
			unobserved (latent) components?\
\
* **Stage 3:** What are our prior beliefs about the parameters
			controlling the components in the model?


## Stage 1: The data generating process

How is our \textcolor{red}{data (\(\boldsymbol{y}\))} generated from the \textcolor{red}{underlying components	(\(\boldsymbol{x}\))} and \textcolor{red}{hyperparameters (\(\boldsymbol{\theta}\))}	in the model:

>- Gaussian response?  (temperature, rainfall,
                        fish weight ...)
>- Count data? (people infected with a disease in each area)
>- Point pattern? (locations of trees in a forest)
>- Binary data? (yes/no response, binary image)
>- Survival data? (recovery time, time to death)

\pause 

This information is placed into our \textcolor{red}{\textcolor{red}{likelihood}
	\(\pi(\boldsymbol{y} | \boldsymbol{x}, \boldsymbol{\theta})\)}


## Stage 1: The data generating process

We assume that *given* the \textcolor{red}{underlying components	(\(\boldsymbol{x}\))} and \textcolor{red}{hyperparameters (\(\boldsymbol{\theta}\))}	the data are independent on each other

$$
\pi(\mathbf{y}|\mathbf{x},\theta) = \prod_{i\in\cal{I}}\pi(y_i|x_{i},\theta)
$$
\pause

\textcolor{blue}{This implies that all the dependence structure in the data is explained in Stage II !!}

\pause
Can you think of a model that does not respect this condition?

## Stage 2: The dependence structure

The underlying \textcolor{red}{unobserved components \(\boldsymbol{x}\)} are called \textcolor{red}{\bf{latent} components} and can be:

* Fixed effects for covariates
* Unstructured random effects (individual effects, group effects)
* Structured random effects (AR(1), regional effects, \ldots)

These are linked to the responses in the likelihood through linear
predictors.


## Stage 3: The hyperparameters

The likelihood and the latent model typically have
hyperparameters that control their behavior.

The \textcolor{red}{hyperparameters \(\boldsymbol{\theta}\)} can include:

\pause

\textcolor{red}{Examples likelihood:}

* Variance of observation noise
* Dispersion parameter in the negative binomial model
* Probability of a zero (zero-inflated models)

\pause

\textcolor{red}{Examples latent model:}

* Variance of unstructured effects
* Correlation of multivariate effects
* Range and variance of spatial effects
*	Autocorrelation parameter

## Example 1: Tokyo rainfall data

Rainfall over 1 mm in the Tokyo area for each calendar day during two
years (1983-84) are registered.
```{r}
data("Tokyo")
pTokyo = ggplot() + geom_point(data = Tokyo, aes(time, y)) +
  ylab("") + xlab("")
pTokyo

mod_tokyo = inla(y~f(time,model = "rw2"), data = Tokyo,
                 family = "binomial",
                 Ntrials = Tokyo$n)
```

## Tokyo rainfall data

Rainfall over 1 mm in the Tokyo area for each calendar day during two
years (1983-84) are registered.

```{r}

dd = data.frame(ii = 1:366,mod_tokyo$summary.fitted.values[,c(1,3,5)])
pTokyo +
   # Custom the Y scales:
  scale_y_continuous(
    # Features of the first axis
    name = "",
    # Add a second axis and specify its features
    sec.axis = sec_axis( trans=~./2, name="Probability")
  )  + geom_line(data = dd, aes(ii, mean*2)) +
  geom_ribbon(data = dd, aes(ii, ymin = X0.025quant*2,
                             ymax = 2 *X0.975quant), alpha = 0.5)
```


## Stage 1: The data

\textcolor{red}{
$$
y_i\mid p_i \sim \text{Binomial}(n_i, p_i),
$$}
for $i=1,2,...,366$

$$
n_{i} = \left\{
 \begin{array}{lr}
1, & \text{for}\; 29\; \text{February}\\
2, & \text{other days}
\end{array}\right.
$$
$$
y_{i} =
\begin{cases}
\{0,1\}, & \text{for}\; 29\; \text{February}\\
\{0,1,2\}, & \text{other days}
 \end{cases}
$$




Linear predictor
$$
logit(p_i) = \eta_i \quad \Leftrightarrow \quad p_i = \frac{1}{1+\exp(-\eta_i)}
$$

* probability of rain on day $i$ depends on $\eta_i$
* the likelihood has no hyperparameters $\theta$



## Stage 2: The latent model


$$
\eta_i = \alpha +  u_i + v_i
$$
where

>- $\alpha$ is a global mean (Gaussian)

>- $\mathbf{u}$ is a AR-model like
$$
u_i  = \phi u_{i-1} + \epsilon_i, \qquad \epsilon_i\sim\mathcal{N}(0, \sigma^2_{\epsilon})
$$
with parameters $(\phi, \sigma_{\epsilon})$

>- $\mathbf{v}$ can be a unstructured term/"random effect"/slowly varying trend. Typically Gaussian with precision parameter $\tau_v$ controlling the smoothness.

\pause
This gives the latent model $\mathbf{x} = (\alpha, \mathbf{u}, \mathbf{v}, \mathbf{\eta})\sim\mathcal{N}(0, \mathbf{Q}^{-1}(\theta))$.

## Stage 3: Hyperparameters


Hyperparameters control the smoothness of the effects in the latent model

$$
\theta = (\phi, \sigma_{\epsilon}, \sigma_v)
$$

## The model
We can write the model as

$$
\begin{aligned}
\theta & \sim \pi(\theta)\\
\mathbf{x}|\theta& \sim \pi(\mathbf{x}|\theta)\\
\mathbf{y}|\mathbf{x},\theta & \sim \prod_i\pi(y_i|\eta_i,\theta)
\end{aligned}
$$


## Precision matrix

```{r, out.width = "80%"}
# All defaults
include_graphics("graphics/Q2.pdf")
```


## Example: disease mapping

We observed larynx cancer mortality counts for males
in 544 district of Germany from 1986 to 1990
and want to understand the spatial distribution and the inpact of covariates.

:::::: {.cols data-latex=""}

::: {.col data-latex="{0.55\textwidth}"}
* \(y_i\):	The count at location \(i\).
* \(E_i\): An offset; expected number of cases in district $i$.
*	\(c_i\): A covariate (level of smoking consumption) at  \(i\)
* \(\boldsymbol{s}_i\):	spatial location \(i\) .
:::

::: {.col data-latex="{0.05\textwidth}"}

<!-- an empty Div (with a white space), serving as
a column separator -->
:::

::: {.col data-latex="{0.4\textwidth}"}
```{r, out.width = "110%"}
source(system.file("demodata/Bym-map.R", package="INLA"))
source("../Rfunctions.R")
data(Germany)
# Load data
Germany$region.struct = Germany$region
g <- system.file("demodata/germany.graph" , package = "INLA")
my.germany.map(Germany$Y/Germany$E, autoscale = F)
```
:::
::::::


## Bayesian disease mapping

>- **Stage 1:**
			We choose a Poisson distribution for the responses, so that
$$
				y_i \mid \eta_i \sim \text{Poisson}(E_i\exp(\eta_i)))
$$
>-	 **Stage 2:**  \(\eta_i\) is a linear function of the latent
                        components: a covariate $c_i$, a spatially
                        structured effect $\mathbf{u}$, an unstructured effect $\mathbf{v}$
			likelihood by
$$
				\eta_i = \mu+ \beta\ c_i + u_i + v_i
$$
>- **Stage 3:**
>   - $\tau_u$:	Precision parameter for the structured effect
> 	- $\tau_v$:	Precision parameter for the unstructured effect
\pause
\

The latent field is \textcolor{red}{$\boldsymbol{x} = (\mu, \beta,\mathbf{u},\mathbf{v})$}, the hyperparameters are \textcolor{red}{ $\boldsymbol{\theta} = (\tau_u,\tau_v)$}, and must be given a prior.


## The model
We can write the model as

$$
\begin{aligned}
\theta & \sim \pi(\theta)\\
\mathbf{x}|\theta& \sim \pi(\mathbf{x}|\theta)\\
\mathbf{y}|\mathbf{x},\theta & \sim \prod_i\pi(y_i|\eta_i,\theta)
\end{aligned}
$$

Identical as the one before!!!!

## Precision Matrix

```{r, out.width = "80%"}
# All defaults
include_graphics("graphics/Q3.pdf")
```


# Latent Gaussian models

## What have we learned so far

Models of the kind:
$$
\begin{aligned}
\theta & \sim \pi(\theta)\\
\mathbf{x}|\theta& \sim \pi(\mathbf{x}|\theta) = \mathcal{N}(0, \mathbf{Q}^{-1}(\theta))\\
\mathbf{y}|\mathbf{x},\theta & \sim \prod_i\pi(y_i|\eta_i,\theta)
\end{aligned}
$$
occurs in many, seemingly unrelated, statistical models.


We call this   \textcolor{red}{\bf Latent Gaussian models}.


## Other example of LGM

* Generalised linear (mixed) models
* Stochastic volatility
* Generalised additive (mixed) models
* Measurement error models
* Spline smoothing 
* Semiparametric regression
* Space-varying (semiparametric) regression models
* Disease mapping
* Log-Gaussian Cox-processes
* Model-based geostatistics (*)
* Spatio-temporal models
* Survival analysis
* +++


## Characteristics of LGM


* The  \textcolor{red}{latent part} of the hierarchical	model is \textcolor{red}{Gaussian}:
$$
\boldsymbol{x} | \boldsymbol{\theta} \sim N(0, {Q}^{-1}(\theta))
$$

* The expected value is $\boldsymbol{0}$

* The *precision* matrix (inverse covariance matrix) is
			${Q}(\theta)$


## The general set-up

The mean of the observation \(i\), \(\mu_i\), is connected to the linear predictor, \(\eta_i\), through a link function \(g\),
$$
\eta_i = g(\mu_i) = \mu + \boldsymbol{z}_i^\top \boldsymbol{\beta}+\sum_{\gamma} w_{\gamma, i} f_\gamma(c_{\gamma,i})+v_i, \quad i = 1,2,\ldots,n
$$

where
$$
\begin{aligned}
\mu &: \text{Intercept}\\
		\boldsymbol{\beta} &: \text{Fixed effects of covariates \(\boldsymbol{z}\)}\\
		\{f_\gamma(\cdot)\} &: \text{Non-linear/smooth effects of covariates \(\boldsymbol{c}\)}\\
		\{w_{\gamma,i}\} &: \text{Known weights defined for each observed data point}\\
		\boldsymbol{v} &: \text{Unstructured error terms}
\end{aligned}
$$




## Specification of the latent field

>- Collect all parameters (random variables) in the \textcolor{red}{latent field}
			$\mathbf{x} =\{\mu, {\beta}, \{f_\gamma(\cdot)\}, {\eta}\}$.
>- A latent Gaussian model is obtained by assigning Gaussian
			priors to all elements of $\mathbf{x}$.
>- Very flexible due to many different forms of the unknown
			functions $\{f_\gamma(\cdot)\}$:
>- \textcolor{red}{Hyperparameters} account for variability and length/strength	of dependence


## Flexibility through \(f\)-functions

The functions \(\{f_\gamma\}\) in the linear predictor make it possible	to capture very different types of random effects in the same framework:

* \(f(\texttt{time})\):\, For example, an AR(1) process,
			RW1 or RW2
* \(f(\texttt{spatial location})\):\, For example, a
			Matern field
* \(f(\texttt{covariate})\):\, For example, a RW1 or RW2 on
			the covariate values
* \(f(\texttt{time}, \texttt{spatial location})\) can be a
			spatio-temporal effect
*	And much more

## Additivity

* One of the most useful features of the framework is the additivity.
* Effects can easily be removed and added without difficulty.
* Each component might add a new latent part and might add new hyperparameters, but the modelling framework and computations stay the same.

\pause
\textcolor{red}{OBS:} The *linear* predictor needs to stay linear!! So effects can be added but not multiplied
\

Why??


## A small point to think about
From a Bayesian point of view fixed effects and random effects
	are all the same.

* Fixed effects are also random
* They only differ in the prior we put on them




## So...which model fit the INLA framework??

1. Latent **Gaussian** model
2. The latent field has a sparse precision matrix (Markov properties)
3. The data are conditionally independent given the latent field
4. The predictor is linear

## 
\small
Assume that, given $\eta = (\eta_1,\dots,\eta_n)$ the observations $y = (y_1,\dots,y_n)$  are independent and Poisson  distributed with parameter $\lambda_ i = \exp(\eta_i)$ i.e.
$$
y_i|\eta_i =\text{Poisson}(\lambda_i); i = 1,\dots,n
$$
\
 1. $\eta_i=\alpha+\beta x_i+U_i$ where
$$
\begin{aligned}
\alpha,\beta & \sim\mathcal{N}(0,1)\\
U_i & \sim \mathcal{N}(0,1) \text{ for } i = 1,\dots,n
\end{aligned}
$$
2. $\eta_i=\alpha+\beta x_i+V_i$ where
$$
\begin{aligned}
\alpha,\beta & \sim\mathcal{N}(0,1)\\
V_i & \sim \text{Bernoulli}(0.4) \text{ for } i = 1,\dots,n
\end{aligned}
$$
3. $\eta_i=\alpha+\beta x_i$ where
$$
\begin{aligned}
\alpha,\beta & \sim\mathcal{N}(0,1)\\
\end{aligned}
$$
4.  $\eta_i=\alpha+\beta x_i + U_iV_i$ where
$$
\begin{aligned}
\alpha,\beta & \sim\mathcal{N}(0,1)\\
U_i & \sim \mathcal{N}(0,1) \text{ for } i = 1,\dots,n\\
V_i & \sim \mathcal{N}(0,1) \text{ for } i = 1,\dots,n
\end{aligned}
$$
\normalsize


## Why precision matrix
